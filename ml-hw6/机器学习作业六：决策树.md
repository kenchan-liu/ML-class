## 机器学习作业六：决策树

决策树(Decision Tree) 是在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于零的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。由于这种决策分支画成图形很像一棵树的枝干，故称决策树。在机器学习中，决策树是一个预测模型，它代表的是对象属性与对象值之间的一种映射关系。Entropy = 系统的凌乱程度，使用算法ID3, C4.5生成树算法使用熵。这一度量是基于信息学理论中熵的概念。

决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。

决策树算法的流程图如图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190720210526493.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly94aW9uZ3lpbWluZy5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70#pic_center)

### 编程实现id3

我们在编程实现的时候，定义一个节点表示一个属性，定义一棵树，包含不同的节点来表示决策树的构建，利用已知的信息来构建决策树，选择信息熵度量来选择树中的不同节点。选择节点的关键在于信息熵增益。

首先来看信息熵的概念（Shannon entropy）：
$$
H(x) = -\sum p(x_i)log p(x_i)
$$
条件熵则是
$$
H(Y|X)=\sum p(x)H(Y|X=x)
$$
使用信息熵和条件熵的差就可以得到一个特征对数据集的信息增益。

~~~python
##id3决策树


class Node:#the node of the decision tree
    Ai = None
    Avalue = None           #value
    child_list = []          #children
    class_y = None           #classification
    print_content = None
    best_gain = None

class DecisionTree:
    root = None
    labels = []
    attr_value_set = {}

    def __init__(self, data, labels):
        self.root = Node()
        self.labels = labels
        for label in self.labels:
            col_num = labels.index(label)
            col = [a[col_num] for a in data]
            self.attr_value_set[label] = set(col)

    def calculate_entropy(self, data):
        # 数据量
        row_num, col_num = np.shape(data)
        label_count = {}
        for row in data:
            current_label = row[-1]
            if current_label not in label_count.keys():
                label_count[current_label] = 0
            label_count[current_label] += 1
        entropy = 0
        # 计算类别的熵
        for key in label_count:
            prob = float(label_count[key]/row_num)
            entropy -= prob*math.log2(prob)
        return entropy

    # 找得类别中最多的
    def Main(self, class_y):
        y_count = {}
        for y in class_y:
            if y not in y_count.keys():
                y_count[y] = 0
            y_count[y] += 1
        max_count = 0
        max_key = class_y[0]
        for key in y_count:
            if y_count[key] > max_count:
                max_count = y_count[key]
                max_key = key
        return max_key

    # 划分数据集，根据属性ai,获取值为ai_v的数据集data_v
    # i：第i个属性（这里data第i列）
    # v: ai第v个取值
    def SplitData(self, data, i, v):
        data_v = []
        for row in data:
            if row[i] == v:
                reduced_row = row[:i]
                reduced_row.extend(row[i + 1:])
                data_v.append(reduced_row)
        return data_v

    # 最优划分属性，根据shannon entropy & 信息增益
    def find_best_attr(self, data):
        attr_num = len(data[0]) - 1
        ent_d = self.calculate_entropy(data)
        # 增益
        best_gain = 0
        # 最优属性序号(0~len(data)-1)
        best_attr = -1
        for i in range(attr_num):
            # 属性ai所有取值，data第i列
            attr_i = [a[i] for a in data]
            # 属性ai所有取值
            attr_iv_set = set(attr_i)

            # 属性ai的v个取值的熵求和(例：ai表示色泽，ai_v=青绿，v=0,1,2...)
            # sum: |D^v|/|D|*ent(D^v)(v=0,1,2...)
            sum_ent_dv = 0
            for attr_iv in attr_iv_set:
                data_v = self.SplitData(data, i, attr_iv)
                prob = len(data_v)/len(data)
                sum_ent_dv += prob*self.calculate_entropy(data_v)
            gain_v = ent_d - sum_ent_dv
            # 获取最大增益和对应属性
            if gain_v > best_gain:
                best_gain = gain_v
                best_attr = i
        return [best_attr, best_gain]

    def create_tree(self, data, labels, node=None):
        if node is None:
            node = self.root
        # get class
        class_y = [cls[-1] for cls in data]
        if class_y.count(class_y[0]) == len(class_y):
            node.class_y = class_y[0]
            return
        # attribute none
        if labels is None or len(labels) == 0:
            node.class_y = self.Main(class_y)
            return
        # 从属性中找到最优划分属性(ID3算法)
        best_attr, best_gain = self.find_best_attr(data)
        node.Ai = labels[best_attr]
        node.best_gain = best_gain
        # 最优属性ai所有样本值，data第i列
        # 最优属性ai所有取值
        attr_iv_set = self.attr_value_set[node.Ai]
        for attr_iv in attr_iv_set:
            # 为node生成一个分支节点，data_v表示data在属性attr_i取值为attr_iv的样本集合
            child_node = Node()
            child_node.child_list = []
            child_node.print_content = str(node.Ai)+str(attr_iv)
            child_node.Avalue = attr_iv
            node.child_list.append(child_node)
            data_v = self.SplitData(data, best_attr, attr_iv)
            # data_v是空，标记为叶子节点，分类属于data中节点最多的
            if data_v is None or len(data_v) == 0:
                class_v_y = [cls[-1] for cls in data]
                child_node.class_y = self.Main(class_v_y)
            else:
                label_v = labels[:best_attr] + labels[best_attr + 1:]
                self.create_tree(data_v, label_v, child_node)
        return

    # 给定属性，进行预测，树前序遍历
    def predict(self, x, node=None):
        if node is None:
            node = self.root
        if node.class_y is not None:
            print(node.class_y)
            return node
        # 节点对应属性位置
        ai_index = self.labels.index(node.Ai)
        ai_v = x[ai_index]
        print(node.Ai)
        if len(node.child_list) > 0:
            for child_node in node.child_list:
                leaf = None
                if child_node.Avalue == ai_v:
                    print(child_node.print_content)
                    leaf = self.predict(x, child_node)
                if leaf is not None:
                    return leaf


    def bfs_tree(self):
        queue = []
        if self.root is not None:
            queue.append(self.root)
        while queue is not None and len(queue) > 0:
            # 每层节点数
            level_num = len(queue)
            for i in range(level_num):
                if len(queue[0].child_list) > 0:
                    for node in queue[0].child_list:
                        queue.append(node)
                print_content = queue[0].print_content if queue[0].print_content is not None else ""
                if queue[0].Ai is not None:
                    print(print_content, queue[0].Ai, queue[0].best_gain, end=' ')
                    print(",", end=' ')
                if queue[0].class_y is not None:
                    print(print_content, queue[0].class_y, end=' ')
                    print(",", end=' ')
                del queue[0]
            print(" ")


~~~

上面是使用id3算法来构建决策树的代码，定义了一个decision tree类。

从根节点开始，计算所有可能特征的信息增益，选择信息增益最大的特征作为划分改节点的特征，根据改特征的不同取值建立子节点，对子节点递归调用该方法，直到停止。

迭代停止的条件是

1)当前节点所有样本都属于同一类别

2）当前节点的所有属性值都相同，没有剩余属性可以用来进一步划分样本

3）达到最大树深

4）达到叶子节点的最小样本数

举例：

![image-20211204211511304](C:\Users\kentl\AppData\Roaming\Typora\typora-user-images\image-20211204211511304.png)

id3选择递归建树的关键在于信息熵增益。

上面的代码在输入训练信息构建树之后，可以查看树的结构：

![image-20211204211650437](C:\Users\kentl\AppData\Roaming\Typora\typora-user-images\image-20211204211650437.png)

在命令行界面难以展示树的结构，上面的文字信息用树的形式画出来如图：

![image-20211204211724022](C:\Users\kentl\AppData\Roaming\Typora\typora-user-images\image-20211204211724022.png)

根据已经构建好的树来进行预测:

![image-20211204211838462](C:\Users\kentl\AppData\Roaming\Typora\typora-user-images\image-20211204211838462.png)

### id3决策树测试结果

经过测试，在watermelondata test1.csv十个测试集中正确了7个，**准确率为70%**。

### c4.5算法

c4.5算法和id3的算法区别在于，**ID3用信息增益，C4.5用信息增益率**。

计算信息熵的公式在上面已经写过了，信息增益率需要考虑属性分裂信息度量。
$$
IGR = gain/H
$$
信息分裂度量计算方式如下：
$$
H(i) = \sum p_i*log(p_i)
$$
因此只需要在上面的基础上稍作修改就行

主要关注一下找最佳分类属性的类方法就行：

~~~python
def find_best_attr(self, data):
        attr_num = len(data[0]) - 1
        ent_d = self.calculate_entropy(data)
        # 增益
        best_gain = 0
        # 最优属性序号(0~len(data)-1)
        best_attr = -1
        for i in range(attr_num):
            # 属性ai所有取值，data第i列
            attr_i = [a[i] for a in data]
            # 属性ai所有取值
            attr_iv_set = set(attr_i)

            # 属性ai的v个取值的熵求和
            sum_ent_dv = 0
            intr = 0
            for attr_iv in attr_iv_set:
                data_v = self.SplitData(data, i, attr_iv)
                prob = len(data_v)/len(data)
                intr -= prob*math.log2(prob)
                print(intr)
                sum_ent_dv += prob*self.calculate_entropy(data_v)
            gain_v = ent_d - sum_ent_dv
            IGRv = 0
            if(intr!=0):
                IGRv = gain_v/intr
            # 获取最大增益和对应属性
            if IGRv > best_gain:
                best_gain = IGRv
                best_attr = i
        return [best_attr, best_gain]
~~~

但是这里注意到第二个数据集的密度属性不再是离散的，而是一个连续的变量。

在决策树的构建中，我们需要将连续属性离散化，因此我们选择连续变量的边界，将其分成几块，按照连续变量的大小做好分类。这样连续的属性就变成离散的了。

~~~python
for i in range(5):
    if(test_set2[i][4]<0.437):
        test_set2[i][4] = 0.437
    elif(test_set2[i][4]>0.437 and test_set2[i][4]<0.639):
        test_set2[i][4] = 0.639
    elif(test_set2[i][4]>0.639):
        test_set2[i][4] = 0.774
~~~

### c4.5测试结果

![image-20220107222312020](https://s2.loli.net/2022/01/07/T4eKDnBNmbVhIxy.png)

在测试集watermelon-test2测试集上测过生成的新决策树之后，结果正确率**5个对4个**，准确率**80%**。